#!/usr/bin/env python3
"""
Self-Distillation Fine-tuningì„ ìœ„í•œ Learning Rate Scheduler ê°€ì´ë“œ
ìˆ˜ë ´ëœ ëª¨ë¸ì—ì„œ self-distillationì„ ì ìš©í•  ë•Œì˜ ìµœì  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
"""

def analyze_scheduler_options():
    """Fine-tuningì— ì í•©í•œ ìŠ¤ì¼€ì¤„ëŸ¬ ì˜µì…˜ë“¤ì„ ë¶„ì„"""
    
    print("ğŸ¯ Self-Distillation Fine-tuningìš© ìŠ¤ì¼€ì¤„ëŸ¬ ê°€ì´ë“œ")
    print("="*65)
    
    print("\nğŸ” í˜„ì¬ ìƒí™© ë¶„ì„:")
    print("âœ… ìˆ˜ë ´ëœ ëª¨ë¸ì—ì„œ ì‹œì‘ (pre-trained)")
    print("âœ… Self-distillation ì ìš© (fine-tuning)")
    print("âœ… ì•ˆì •ì ì¸ í•™ìŠµ í•„ìš”")
    print("âŒ Noam scheduler: ì²˜ìŒë¶€í„° í•™ìŠµìš©, fine-tuningì— ë¶€ì í•©")
    
    print("\n" + "="*65)
    print("ğŸ“Š ì¶”ì²œ ìŠ¤ì¼€ì¤„ëŸ¬ ì˜µì…˜ë“¤:")
    print("="*65)
    
    print("\nğŸ’¡ Option 1: Constant LR (ê°€ì¥ ì•ˆì „)")
    print("ğŸ“Œ íŠ¹ì§•:")
    print("   - ê³ ì •ëœ í•™ìŠµë¥  ì‚¬ìš©")
    print("   - ê°€ì¥ ì•ˆì •ì ì´ê³  ì˜ˆì¸¡ ê°€ëŠ¥")
    print("   - Fine-tuningì˜ í‘œì¤€ ë°©ë²•")
    print("ğŸ“Œ ì¶”ì²œ ì„¤ì •:")
    print("   - Learning Rate: 1e-5 ~ 5e-5")
    print("   - Weight Decay: 1e-6 ~ 1e-4")
    print("   - No warmup needed")
    
    print("\nğŸ’¡ Option 2: Cosine Annealing (ë¶€ë“œëŸ¬ìš´ ê°ì†Œ)")
    print("ğŸ“Œ íŠ¹ì§•:")
    print("   - ì½”ì‚¬ì¸ í•¨ìˆ˜ë¡œ ë¶€ë“œëŸ½ê²Œ ê°ì†Œ")
    print("   - ë§ˆì§€ë§‰ì— ë§¤ìš° ì‘ì€ lrë¡œ ìˆ˜ë ´")
    print("   - ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒì— ë„ì›€")
    print("ğŸ“Œ ì¶”ì²œ ì„¤ì •:")
    print("   - Initial LR: 2e-5")
    print("   - T_max: total_epochs (ì „ì²´ ì—í¬í¬ ìˆ˜)")
    print("   - eta_min: 1e-6")
    
    print("\nğŸ’¡ Option 3: Step LR (ë‹¨ê³„ë³„ ê°ì†Œ)")
    print("ğŸ“Œ íŠ¹ì§•:")
    print("   - íŠ¹ì • ì—í¬í¬ë§ˆë‹¤ lrì„ ê³ ì • ë¹„ìœ¨ë¡œ ê°ì†Œ")
    print("   - ì œì–´í•˜ê¸° ì‰½ê³  í•´ì„ ìš©ì´")
    print("   - Validation loss plateauì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥")
    print("ğŸ“Œ ì¶”ì²œ ì„¤ì •:")
    print("   - Initial LR: 3e-5")
    print("   - Step size: 10-15 epochs")
    print("   - Gamma: 0.5 (50% ê°ì†Œ)")
    
    print("\nğŸ’¡ Option 4: Reduce on Plateau (ì ì‘ì )")
    print("ğŸ“Œ íŠ¹ì§•:")
    print("   - Validation lossê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ lr ê°ì†Œ")
    print("   - ìë™ìœ¼ë¡œ ìµœì  ì‹œì ì— lr ì¡°ì •")
    print("   - Self-distillation ìˆ˜ë ´ íŠ¹ì„±ì— ì í•©")
    print("ğŸ“Œ ì¶”ì²œ ì„¤ì •:")
    print("   - Initial LR: 2e-5")
    print("   - Patience: 3-5 epochs")
    print("   - Factor: 0.5")
    print("   - Min LR: 1e-6")

def recommend_best_option():
    """Self-distillationì— ê°€ì¥ ì í•©í•œ ì˜µì…˜ ì¶”ì²œ"""
    
    print("\n" + "="*65)
    print("ğŸ† Self-Distillation Fine-tuning ìµœê³  ì¶”ì²œ:")
    print("="*65)
    
    print("\nğŸ¥‡ 1ìˆœìœ„: Reduce on Plateau")
    print("ì´ìœ :")
    print("   âœ… Self-distillationì€ loss landscapeê°€ ë³µì¡í•¨")
    print("   âœ… Teacher-student ìˆ˜ë ´ ì†ë„ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ")
    print("   âœ… Validation loss ê¸°ë°˜ ìë™ ì¡°ì •ì´ ìµœì ")
    print("   âœ… ê³¼ë„í•œ lr ê°ì†Œ ë°©ì§€")
    
    print("\nğŸ¥ˆ 2ìˆœìœ„: Constant LR")
    print("ì´ìœ :")
    print("   âœ… ê°€ì¥ ì•ˆì „í•˜ê³  ì•ˆì •ì ")
    print("   âœ… Fine-tuningì˜ ê²€ì¦ëœ ë°©ë²•")
    print("   âœ… Hyperparameter tuning ë¶€ë‹´ ì ìŒ")
    print("   âœ… Reproducible results")
    
    print("\nğŸ¥‰ 3ìˆœìœ„: Cosine Annealing")
    print("ì´ìœ :")
    print("   âœ… ë¶€ë“œëŸ¬ìš´ ìˆ˜ë ´")
    print("   âœ… ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ")
    print("   âš ï¸  Total epochs ë¯¸ë¦¬ ì •í•´ì•¼ í•¨")

def provide_implementation_code():
    """êµ¬í˜„ ì½”ë“œ ì˜ˆì‹œ ì œê³µ"""
    
    print("\n" + "="*65)
    print("ğŸ’» êµ¬í˜„ ì½”ë“œ (train.py ìˆ˜ì •):")
    print("="*65)
    
    print("\nğŸ”§ Option 1: Reduce on Plateau (ì¶”ì²œ)")
    print("""
# train.pyì—ì„œ Noam optimizer ëŒ€ì‹ :
import torch.optim as optim

# Adam optimizer ì‚¬ìš©
optimizer = optim.Adam(
    model.parameters(),
    lr=2e-5,
    weight_decay=1e-4,
    betas=(0.9, 0.999),
    eps=1e-8
)

# ReduceLROnPlateau scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',           # validation loss ê¸°ì¤€
    factor=0.5,          # 50% ê°ì†Œ
    patience=3,          # 3 epochs ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ê°ì†Œ
    min_lr=1e-6,         # ìµœì†Œ í•™ìŠµë¥ 
    verbose=True         # ë¡œê·¸ ì¶œë ¥
)
""")
    
    print("\nğŸ”§ Option 2: Constant LR")
    print("""
# Adam optimizer with constant LR
optimizer = optim.Adam(
    model.parameters(),
    lr=3e-5,             # ê³ ì • í•™ìŠµë¥ 
    weight_decay=1e-4
)

# No scheduler needed (ë˜ëŠ” dummy scheduler)
scheduler = None
""")
    
    print("\nğŸ”§ Option 3: Cosine Annealing")
    print("""
optimizer = optim.Adam(
    model.parameters(),
    lr=2e-5,
    weight_decay=1e-4
)

scheduler = optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=50,            # ì´ ì—í¬í¬ ìˆ˜
    eta_min=1e-6         # ìµœì†Œ í•™ìŠµë¥ 
)
""")

def suggest_learning_rates():
    """Self-distillationì— ì í•©í•œ í•™ìŠµë¥  ë²”ìœ„ ì œì•ˆ"""
    
    print("\n" + "="*65)
    print("ğŸ“ˆ Self-Distillation Fine-tuning í•™ìŠµë¥  ê°€ì´ë“œ:")
    print("="*65)
    
    print("\nğŸ¯ ì¶”ì²œ í•™ìŠµë¥  ë²”ìœ„:")
    print("ğŸ“Œ Conservative (ì•ˆì „í•œ ì‹œì‘): 1e-5")
    print("ğŸ“Œ Moderate (í‘œì¤€ ì„¤ì •): 2e-5 ~ 3e-5")
    print("ğŸ“Œ Aggressive (ë¹ ë¥¸ ìˆ˜ë ´): 5e-5")
    
    print("\nâš ï¸  ì£¼ì˜ì‚¬í•­:")
    print("âŒ 1e-4 ì´ìƒ: ë„ˆë¬´ ë†’ìŒ, ë°œì‚° ìœ„í—˜")
    print("âŒ 1e-6 ì´í•˜: ë„ˆë¬´ ë‚®ìŒ, ìˆ˜ë ´ ë„ˆë¬´ ëŠë¦¼")
    print("âœ… 2e-5: ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì— ì•ˆì •ì ")
    
    print("\nğŸ”¬ ì‹¤í—˜ ìˆœì„œ:")
    print("1ï¸âƒ£  ReduceLROnPlateau + 2e-5ë¡œ ì‹œì‘")
    print("2ï¸âƒ£  5-10 epochs í›„ loss íŒ¨í„´ ê´€ì°°")
    print("3ï¸âƒ£  í•„ìš”ì‹œ ì´ˆê¸° lr ì¡°ì • (1e-5 or 3e-5)")
    print("4ï¸âƒ£  Patience, factor ë“± ì„¸ë¶€ ì¡°ì •")

def provide_train_sd_modifications():
    """train_sd.sh ìˆ˜ì •ì‚¬í•­ ì œì•ˆ"""
    
    print("\n" + "="*65)
    print("ğŸ› ï¸  train_sd.sh ìˆ˜ì • ë°©ì•ˆ:")
    print("="*65)
    
    print("\nğŸ“ ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„° ì¶”ê°€:")
    print("""
# Learning Rate Settings for Fine-tuning
use_noam_scheduler=false      # Noam ëŒ€ì‹  ë‹¤ë¥¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
optimizer_type="adam"         # "adam" or "adamw"
learning_rate=2e-5           # ê³ ì • í•™ìŠµë¥  ë˜ëŠ” ì´ˆê¸° í•™ìŠµë¥ 
scheduler_type="plateau"      # "constant", "plateau", "cosine", "step"
scheduler_patience=3          # ReduceLROnPlateauìš©
scheduler_factor=0.5          # LR ê°ì†Œ ë¹„ìœ¨
min_learning_rate=1e-6       # ìµœì†Œ í•™ìŠµë¥ 
""")
    
    print("\nğŸ“ train.py í˜¸ì¶œë¶€ ìˆ˜ì •:")
    print("""
python3 ./conformer_ctc_sd/train.py \\
    --use-noam-scheduler $use_noam_scheduler \\
    --optimizer-type $optimizer_type \\
    --learning-rate $learning_rate \\
    --scheduler-type $scheduler_type \\
    --scheduler-patience $scheduler_patience \\
    --scheduler-factor $scheduler_factor \\
    --min-learning-rate $min_learning_rate \\
    # ... ê¸°ì¡´ íŒŒë¼ë¯¸í„°ë“¤
""")

if __name__ == "__main__":
    analyze_scheduler_options()
    recommend_best_option()
    provide_implementation_code()
    suggest_learning_rates()
    provide_train_sd_modifications()
    
    print("\n" + "="*65)
    print("ğŸ¯ ìµœì¢… ì¶”ì²œ ìš”ì•½:")
    print("="*65)
    print("1ï¸âƒ£  ReduceLROnPlateau + Adam (lr=2e-5)")
    print("2ï¸âƒ£  Patience=3, Factor=0.5, Min_lr=1e-6")
    print("3ï¸âƒ£  Weight_decay=1e-4")
    print("4ï¸âƒ£  5-10 epochs í›„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
    print("5ï¸âƒ£  í•„ìš”ì‹œ ì´ˆê¸° lrì„ 1e-5 ë˜ëŠ” 3e-5ë¡œ ì¡°ì •")
    print("\nğŸ’¡ ì´ ì„¤ì •ìœ¼ë¡œ ì•ˆì •ì ì´ê³  íš¨ê³¼ì ì¸ self-distillationì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
