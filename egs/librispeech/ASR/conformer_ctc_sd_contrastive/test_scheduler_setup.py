#!/usr/bin/env python3
"""
Scheduler ì„¤ì • í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ReduceLROnPlateauì™€ Constant LR ì„¤ì •ì´ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë˜ì—ˆëŠ”ì§€ í…ŒìŠ¤íŠ¸
"""

import sys
import torch
import torch.nn as nn
import torch.optim as optim

def test_scheduler_configurations():
    """ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •ì„ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª Scheduler Configuration Test")
    print("="*50)
    
    # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±
    model = nn.Linear(10, 1)
    
    print("\n1ï¸âƒ£  Testing ReduceLROnPlateau Configuration:")
    print("-" * 40)
    
    # ReduceLROnPlateau ì„¤ì •
    optimizer_plateau = optim.Adam(
        model.parameters(),
        lr=2e-5,
        weight_decay=1e-4,
        betas=(0.9, 0.999),
        eps=1e-8
    )
    
    scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_plateau,
        mode='min',
        factor=0.5,
        patience=3,
        min_lr=1e-6,
        verbose=True
    )
    
    print(f"âœ… Initial LR: {optimizer_plateau.param_groups[0]['lr']:.2e}")
    print(f"âœ… Scheduler Type: {type(scheduler_plateau).__name__}")
    print(f"âœ… Mode: min, Factor: 0.5, Patience: 3, Min LR: 1e-6")
    
    # ëª‡ ë²ˆì˜ ìŠ¤í… ì‹œë®¬ë ˆì´ì…˜
    print("\nğŸ“ˆ Simulating validation loss steps:")
    validation_losses = [2.5, 2.3, 2.4, 2.4, 2.4, 2.4]  # No improvement for 3 steps
    
    for i, val_loss in enumerate(validation_losses):
        print(f"   Step {i+1}: val_loss={val_loss:.1f}, lr={optimizer_plateau.param_groups[0]['lr']:.2e}")
        scheduler_plateau.step(val_loss)
        if i < len(validation_losses) - 1:
            print(f"            â†’ Next lr={optimizer_plateau.param_groups[0]['lr']:.2e}")
    
    print(f"\nâœ… Final LR after patience exhausted: {optimizer_plateau.param_groups[0]['lr']:.2e}")
    
    print("\n2ï¸âƒ£  Testing Constant LR Configuration:")
    print("-" * 40)
    
    # Constant LR ì„¤ì •
    optimizer_constant = optim.Adam(
        model.parameters(),
        lr=2e-5,
        weight_decay=1e-4
    )
    
    print(f"âœ… Constant LR: {optimizer_constant.param_groups[0]['lr']:.2e}")
    print(f"âœ… No scheduler needed")
    print(f"âœ… LR remains constant throughout training")
    
    # ëª‡ ì—í¬í¬ ì‹œë®¬ë ˆì´ì…˜
    print("\nğŸ“ˆ Simulating epochs with constant LR:")
    for epoch in range(1, 6):
        print(f"   Epoch {epoch}: lr={optimizer_constant.param_groups[0]['lr']:.2e}")
    
    print("\n3ï¸âƒ£  Testing Noam Scheduler (for comparison):")
    print("-" * 40)
    
    # Noam scheduler ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ import ì—†ì´)
    print("âœ… Noam Scheduler characteristics:")
    print("   - Warmup phase: lr increases linearly")
    print("   - Decay phase: lr decreases as step^(-0.5)")
    print("   - Self-contained: handles scheduling internally")
    print("   - Good for: Training from scratch")
    
    print("\n" + "="*50)
    print("ğŸ“‹ Scheduler Comparison Summary:")
    print("="*50)
    
    print("\nğŸ† ReduceLROnPlateau:")
    print("   âœ… Adapts to validation loss")
    print("   âœ… Best for fine-tuning")
    print("   âœ… Prevents overfitting")
    print("   âš ï¸  Requires validation monitoring")
    
    print("\nğŸ¥ˆ Constant LR:")
    print("   âœ… Simple and stable")
    print("   âœ… Easy to reproduce")
    print("   âœ… Good for fine-tuning")
    print("   âš ï¸  May need manual adjustment")
    
    print("\nğŸ¥‰ Noam Scheduler:")
    print("   âœ… Good for training from scratch")
    print("   âœ… Proven for Transformer models")
    print("   âŒ Not suitable for fine-tuning")
    print("   âŒ Fixed schedule, not adaptive")

def test_argument_parsing():
    """ì¸ì íŒŒì‹± í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜"""
    
    print("\n" + "="*50)
    print("ğŸ”§ Argument Parsing Test:")
    print("="*50)
    
    test_cases = [
        {
            "name": "ReduceLROnPlateau Configuration",
            "args": {
                "scheduler_type": "plateau",
                "base_lr": 2e-5,
                "scheduler_patience": 3,
                "scheduler_factor": 0.5,
                "min_lr": 1e-6,
            }
        },
        {
            "name": "Constant LR Configuration", 
            "args": {
                "scheduler_type": "constant",
                "base_lr": 3e-5,
                "scheduler_patience": 3,  # unused
                "scheduler_factor": 0.5,  # unused
                "min_lr": 1e-6,          # unused
            }
        },
        {
            "name": "Noam Scheduler Configuration (legacy)",
            "args": {
                "scheduler_type": "noam",
                "lr_factor": 5.0,
                "warm_step": 10000,
                "attention_dim": 512,
            }
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{i}ï¸âƒ£  {test_case['name']}:")
        for key, value in test_case['args'].items():
            print(f"   --{key.replace('_', '-')}: {value}")

if __name__ == "__main__":
    test_scheduler_configurations()
    test_argument_parsing()
    
    print("\n" + "="*50)
    print("ğŸ¯ Ready for Self-Distillation Fine-tuning!")
    print("="*50)
    print("âœ… ReduceLROnPlateau: train_sd.shì—ì„œ scheduler_type='plateau'")
    print("âœ… Constant LR: train_sd.shì—ì„œ scheduler_type='constant'")
    print("âœ… ê¸°ë³¸ ì„¤ì •: plateau with lr=2e-5, patience=3, factor=0.5")
    print("\nğŸš€ ì´ì œ train_sd.shë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
